#pragma once
// ============================================================================
//  localization/vision_localizer.h — 视觉定位（用 AprilTag 当"GPS"）
// ============================================================================
//
//  【这个文件干什么？】
//    里程计（odometry）虽然能追踪位置，但时间长了会慢慢"飘"——
//    就像你闭着眼睛走路，走着走着就偏了。
//    视觉定位就是用摄像头看赛场上的 AprilTag 标签来修正偏差，
//    就像你睁眼看一下路标就知道自己到底在哪了。这就是"GPS"的作用！
//
//  【工作流程（6 步）】
//    1. 场地上贴着 AprilTag 标签，每个标签的坐标是已知的
//    2. 摄像头拍照，检测到标签的像素位置和大小
//    3. 从标签大小 → 估算距离（大=近，小=远，就像远处的人看起来小一样）
//    4. 从标签在图片中的水平位置 → 估算方位角
//    5. 已知标签位置 + 距离 + 方位角 → 算出机器人的位置
//    6. 把结果和里程计的位置加权融合（不是直接替换，而是慢慢修正）
//
//  【为什么不只用里程计？】
//    里程计有"累积误差"——走得越远，偏差越大。
//    视觉定位提供"绝对位置"——直接告诉你你在哪，不会累积偏差！
//    两者结合：里程计提供高频平滑更新，视觉提供低频绝对校正。
//
//  【局限性】
//    • 需要能看到至少一个标签（被挡住就没法定位）
//    • 更新速度受摄像头帧率限制（约 15-30 fps）
//    • 距离太远（>2米）精度会下降（标签在图片里太小了）
//
// ============================================================================
#include "localization/odometry.h"

/// 赛场上一个 AprilTag 标签的已知信息
struct FieldTag {
    int    id;      ///< 标签 ID（必须和实际贴的标签上的数字一致）
    double x;       ///< 在赛场上的 X 坐标（米）
    double y;       ///< 在赛场上的 Y 坐标（米）
    double z;       ///< 离地面的高度（米）—— 用来修正距离估算
    double facing;  ///< 标签朝向（弧度）—— 标签表面法线的方向
};

/// 一次视觉定位的结果
struct VisionEstimate {
    double x;           ///< 估算出的机器人 X 坐标（米）
    double y;           ///< 估算出的机器人 Y 坐标（米）
    double heading;     ///< 估算出的航向（弧度，目前直接用里程计的）
    double confidence;  ///< 置信度 [0, 1]（基于距离和标签大小，越大越可信）
    bool   valid;       ///< false = 没有可用的检测结果
};

/// 初始化视觉定位器（在 vision_init() 之后调用）
void vision_localizer_init();

/// 拍照 + 处理所有检测到的标签，返回最佳的位置估算
/// 内部会自动调用 vision_snapshot()
/// @return 位置估算结果（使用前检查 .valid）
VisionEstimate vision_localizer_update();

/// 把视觉定位结果融合到里程计中
/// 使用"互补滤波器"：新位置 = (1-α) × 里程计 + α × 视觉
/// 这样不会因为一次不准的视觉读数就让位置跳来跳去
/// @param estimate  来自 vision_localizer_update() 的结果
void vision_correct_odometry(const VisionEstimate& estimate);

/// Get the number of tags detected in the last update.
int vision_localizer_tag_count();
